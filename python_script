'''
The code are used to assess the potential carbon improvement through adjusting HUMAN practices
First, various images are created, then statistics on the images are made
Note the the codes are not optimized for performance but just used to realize the basic functions
'''


import ee
from functools import partial


ee.Initialize()

#myaccount_name is your account

#data sources
day8_psnnet = ee.ImageCollection("MODIS/006/MOD17A2H")
#soil type comes from harmonized world soil dataset (HWSD, http://www.fao.org/soils-portal/soil-survey/soil-maps-and-databases/harmonized-world-soil-database-v12/en/)
# ids are recoded from the original HWSD code to make sure the code has maximum three digits.
hwsd = ee.Image("users/zongyaosha/soil")
landcoverMOD12Q1 = ee.ImageCollection("MODIS/006/MCD12Q1")
#land forms (Iwahashi) of the world comes from https://esdac.jrc.ec.europa.eu/content/global-landform-classification
landforms = ee.Image("users/zongyaosha/Iwahashi")
#monthly temperture and precipitation
climate = ee.ImageCollection("IDAHO_EPSCOR/TERRACLIMATE")
world_regions = ee.FeatureCollection("USDOS/LSIB_SIMPLE/2017")
scaleSize = 500 #pixel size

# study region: mainland China
#the following should be run first to prepare region of interest(roi)
def extractROI():
    mainlandchina=world_regions.filterMetadata("country_na","equals","China")
    ee.batch.Export.table.toAsset(
      collection=mainlandchina,
      description='MainLandChina',
      assetId= 'MainLandChina'
    )
#run extractROI first to prepare MainLandChina featurecollection
#extractROI()

#now ROI MainLandChina is created and can be referred in all the functions
roi = ee.FeatureCollection("users/zongyaosha/MainLandChina")
climate=climate.filterBounds(roi)
landcoverMOD12Q1=landcoverMOD12Q1.filterBounds(roi)
day8_psnnet=day8_psnnet.filterBounds(roi)
#clip and reproject
landforms_clip = ee.Image("users/zongyaosha/Iwahashi").clip(roi).reproject('EPSG:4326', None, scaleSize)

#clip and reproject
soil_clip = hwsd.clip(roi).reproject('EPSG:4326', None, scaleSize)
neiborhoodSize=40 # Estimated to be 40 pixels=20km
neighborhoodKernel=ee.Kernel.circle(neiborhoodSize)

#percentile 90, target maximum NPP level for distance-constrained zonal analysis
# (zone defined by landform+soil+landcover segmentation unit and a distance)
percentile=ee.Reducer.percentile([90])

# pixel area (unit in hectare) could be different depending on latitude, thus needs to be processed for later statistics
# for pixel at scale=500, pixel area<25 hectare, thus int8 is used
def createPixelArea():
    pa=ee.Image.pixelArea().clip(roi).reproject(crs='EPSG:4326',scale=500).divide(10000).round().int8()
    task = ee.batch.Export.image.toAsset(
               image=pa,
               description='pixelArea',
               assetId='users/myaccount_name/pixelArea',
               scale=500,
               maxPixels=1e13,
               crs='EPSG:4326',
               region=roi.geometry()
     )
    task.start()
#createPixelArea()

#preparing annual average temperature for use in the Miami model which is to compute potential NPP (PNPP)
def averageImage(year):
    sum_tm = None
    for i in range(12):
        climate_tmmx = climate.filterDate(year + '-' + str(i + 1) + '-1', year + '-' + str(i + 1) + '-28').select(
            "tmmx").first()
        climate_tmmn = climate.filterDate(year + '-' + str(i + 1) + '-1', year + '-' + str(i + 1) + '-28').select(
            "tmmn").first()
        r = (climate_tmmx.add(climate_tmmn).divide(2)).rename(['tm_avg'])
        if sum_tm is None:
            sum_tm = r
        else:
            sum_tm = sum_tm.add(r)

    rt = (sum_tm.divide(12)).rename(['tm_avg'])
    return rt.reproject('EPSG:4326', None, scaleSize)

#Compute potential NPP (PNPP)
def computePNPP(year):
    climate_pr = climate.filterDate(year + '-1-1', year + '-12-31').select("pr").sum()
    x1 = ((climate_pr.multiply(-0.000664)).exp().add(-1)).multiply(-3000)
    x22 = (averageImage(year).multiply(-0.119).multiply(0.1).add(1.315)).exp().add(1)
    x2 = ee.Image.constant(3000).divide(x22)
    pnpp = x1.min(x2).multiply(0.45).rename(['pnpp'])
    return pnpp.reproject('EPSG:4326', None, scaleSize)

#Relative NPP, which records the relative observed NPP rectified by PNPP from CLIMATE impact
def getRelativeNPP(year):
    npp = ee.Image("users/myaccount_name/NPP"+str(year)) #getNPP(year)
    pnpp = ee.Image("users/myaccount_name/PNPP"+str(year)) #computePNPP(year)
    bias = npp.subtract(pnpp)  # remove climate effect. The result only indicates relative value between locations
    return bias #.reproject('EPSG:4326', None, scaleSize)

# Segmentation for soil type, landforms and land cover types and assign each unit a 7-digit code
def createUniqueUnits(year):
    #make segmentation unit has unique id (7 digits) defined by combination of landform id, soil type id, and landcover id
    x4 = soil_clip.add(ee.Image.constant(99))  # make soil id to be 3 digits
    x6 = landforms_clip.add(9)  # make land forms id from 1~16 to 10~25 (2 digits)

    # land cover
    landcover = landcoverMOD12Q1.filterDate(year + '-1-1', year + '-12-31').select('LC_Type1').first().clip(
        roi).reproject('EPSG:4326', None, scaleSize)
    x8 = landcover.add(9)  #  make land cover id from 1~17 to 10~26 (2 digits)
    # unique value: landforms*100000+soil*100+landcover, range: 1010010~2969929

    output_landforms_soil = x6.multiply(1000).add(x4)
    # get unique value for the combination of land forms and soil types
    output = output_landforms_soil.multiply(100).add(x8)  # combine output_landforms_soil and landcover
    # unique id=land forms *100000 + soil types*100 + land cover types
    #          10-25               100~449            10~26
    return output.reproject('EPSG:4326', None, scaleSize).rename(['uid'])

#unique_units=createUniqueUnits(2018)

# estimate the constrained distance. This function should be run seperately because it may take days

def estimateDistance(year):
    #save the statistics to a file
    f = open("F:/paper_china/statistics.csv", "a")
    # set 500 random points and evaluate the PCT90 changes when different distance (buffer size) is specified
    for randompt in range(50):
        sample_pnt=ee.FeatureCollection.randomPoints(roi,1,1).geometry()
        #largest distance, here=100pixels=50km
        max_distance_pixels=100
        buffer_Area=sample_pnt.buffer(max_distance_pixels*500) #pixel size=500 m
        landcover = landcoverMOD12Q1.filterDate(str(year)+'-1-1',str(year)+ '-12-31').select('LC_Type1').first().clip(buffer_Area).reproject('EPSG:4326', None, scaleSize)
        #get the area around the current point
        gap= ee.Image("users/myaccount_name/relativeNpp"+str(year)).clip(buffer_Area)
        #zonal analysis needs to select pixels that have the same land cover id as that of the current location
        land_id = (ee.Number(landcover.reduceRegion(ee.Reducer.first(),sample_pnt,1).getNumber("LC_Type1"))).int().getInfo()
        # those ids nnt considered
        if land_id==13 or land_id>=15: # land cover id=13, 15,16,17 are not vegetated
            continue
        # one point, one line showing PCT90 changes with the buffer distance
        gap_with_buffer=""
        for buff in range(max_distance_pixels): # increase distance and check PCT90 response
            geom_buffer=sample_pnt.buffer((buff+1)*500-1)
            data = gap.clip(geom_buffer).mask(landcover.eq(ee.Image.constant(land_id))).reduceRegion(ee.Reducer.percentile([90]),geom_buffer,500)
            dataN = ee.Number(data)
            print((dataN.getInfo())["PsnNet"])
            gap_with_buffer+=(dataN.getInfo())["PsnNet"]+"    "
        f.write("estimate distance:    "+str(year)+"   "+gap_with_buffer+"\n")
    f.close()
    #analyze statistics offline. PCT90 changes with the distance. Either max value or stable value (level off line) is selected to be the distance
    # the constrained distance locates between 15 km- 20 km and we select 20 km (40 pixels) for carbon gap computing

#get carbon gap for each unit based on 90 percentile of focal statistics
# val is the current landform-vegetation-soil (LVS) id
def processGap(val, npp_for_compute,unique_units_img):
    constrained_distance=40 # from estimateDistance result. About 20km or 40 pixels in distance
    filterout = unique_units_img.updateMask(unique_units_img.eq(val)) # only those pixel values equal to val are taken from the DC-zone
    relative_npp_mask = npp_for_compute.updateMask(filterout)

    focal_pct90 = relative_npp_mask.reduceNeighborhood(ee.Reducer.percentile([90]),
                                                       ee.Kernel.circle(constrained_distance)).reproject('EPSG:4326', None, scaleSize)
    # gap calculation
    gap = focal_pct90.subtract(relative_npp_mask)
    return gap  # landforms = ee.Image('LANDSAT/LC8_L1T_TOA/LC81230322014135LGN00').select(['B4'])

#Because getting ee.Reducer.percentile([90] for all locations based on DC-Zonal analysis consomes high GEE resources,
# we have to split the work to many units, each time one combination of soil_id,landform id and vegetation id.
# so we have three loops: loopThroughSoilId, loopThroughlandFormId and loopThroughLandCoverId

# all soil type id list
soil_ids = [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34,
            35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,
            65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 87, 88, 89, 91, 92, 93, 94, 95, 97,
            98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120,
            122, 123, 124, 126, 127, 128, 129, 132, 133, 134, 135, 136, 138, 139, 140, 141, 144, 145, 146, 147, 149,
            150, 151, 152, 153, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173,
            174, 175, 176, 178, 179, 180, 181, 194, 195, 196, 198, 199, 200, 201, 202, 204, 205, 301, 302, 303, 304,
            305, 306, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 323, 324, 325, 326, 327, 328, 329,
            330, 331, 333, 334, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,
            355, 356, 357, 360, 362, 363, 365, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,
            382, 383, 384, 385, 387, 388, 391, 392, 394, 395, 396, 397, 398, 399, 401, 402, 405, 406, 407, 408, 409,
            411, 412, 413, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 435, 437, 438, 440, 441, 442, 446, 447,
            448, 449, 450]

def loopThroughSoilId(relative_npp,unique_units):
    imgList =  list(map(partial(loopThroughlandFormId, relative_npp=relative_npp,unique_units=unique_units), soil_ids))
    return ee.ImageCollection(imgList).reduce(ee.Reducer.sum(),4)

# all landform id list. Note the new id=9+original landform_id, see createUniqueUnits function
landformids = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
def loopThroughlandFormId(soilid,relative_npp,unique_units):
    # unique id=2-digit landform id *100000 + 3-digit soil type id*1000 + 2-digit land cover id
    imgList = list(map(partial(loopThroughLandCoverId, soilid=soilid,relative_npp=relative_npp,unique_units=unique_units), landformids))
    return ee.ImageCollection(imgList).reduce(ee.Reducer.sum(),4) # parallelScale=4 makes sure computing efficiency

# all landcover id list. Note the new landcover id=9+original landcover_id, see createUniqueUnits function
landcoverids=[10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]
def loopThroughLandCoverId(landformid, soilid,relative_npp,unique_units):
    imgList = list(map(partial(getImage, soilid=soilid, landcoverid=landformid,relative_npp=relative_npp,unique_units=unique_units), landcoverids))
    return ee.ImageCollection(imgList).reduce(ee.Reducer.sum(),4)#parallelScale=4 to improve efficiency

def getImage(landformid, soilid, landcoverid,relative_npp,unique_units):
    return processGap(landformid*100000+soilid*100+landcoverid,relative_npp,unique_units).unmask(0)

#Now all the functions have been defined and we save the images for different types to asset store

#calculate relative NPP and save to my asset
def saveRelativeNpp():
    # 2001-2018 totally 18 years. We use one task for each year
    resultmap=[None]*18
    task=[None]*18

    for i in range(1): #1-2018
       year=str(2001+i)
       resultmap[i] =getRelativeNPP(year)
       print("begin saveRelativeNpp task:"+year)
       task[i] = ee.batch.Export.image.toAsset(
           image=resultmap[i].int16(),
           description='relativeNpp'+year,
           assetId='users/myaccount_name/relativeNpp'+year,
           scale=500,
           maxPixels=1e13,
           crs='EPSG:4326',
           region=roi.geometry()
       )
       task[i].start()
       print("end saveRelativeNpp task: "+year)
#uncomment the following  to run it
#saveRelativeNpp()

#Annual observed NPP from MODIS. Accumulative annual value
def getNPP(year):
    npp = (day8_psnnet.filterDate(year + '-1-1', year + '-12-31').select('PsnNet')).sum().multiply(0.1)
    return npp.reproject('EPSG:4326', None, scaleSize)

#Calculate NPP and save to my asset
def saveNPP():
    # from 200
    resultmap=[None]*18
    task=[None]*18
    for i in range(18): #2001-2018
       year=str(2001+i)
       resultmap[i] =getNPP(year)
       print("begin npp task:"+year)
       task[i] = ee.batch.Export.image.toAsset(
           image=resultmap[i].clip(roi).round().int16(),
           description='NPP'+year,
           assetId='users/myaccount_name/NPP'+year,
           scale=500,
           maxPixels=1e13,
           crs='EPSG:4326',
           region=roi.geometry()
       )
       task[i].start()
       print("end npp task: "+year)
#uncomment the following  to run it
#saveNPP()

#Calculate PNPP and save to my asset
def savePNPP():
    resultmap=[None]*18
    task=[None]*18

    for i in range(18): #2001-2018
       year=str(2001+i)
       resultmap[i] =computePNPP(year)
       print("begin pnpp task:"+year)
       task[i] = ee.batch.Export.image.toAsset(
           image=resultmap[i].clip(roi).round().int16(),
           description='PNPP'+year,
           assetId='users/myaccount_name/PNPP'+year,
           scale=500,
           maxPixels=1e13,
           crs='EPSG:4326',
           region=roi.geometry()
       )
       task[i].start()
       print("end pnpp task: "+year)
#uncomment the following  to run it
#savePNPP()


def saveUniqueUnits():
    resultmap=[None]*18
    task=[None]*18
    for i in range(1): #2001-2018
       year=str(2001+i)
       resultmap[i] =createUniqueUnits(year)
       print("begin createUniqueUnits task:"+year)
       task[i] = ee.batch.Export.image.toAsset(
           image=resultmap[i].clip(roi),
           description='UniqueUnits'+year,
           assetId='users/myaccount_name/UniqueUnits'+year,
           scale=500,
           maxPixels=1e13,
           crs='EPSG:4326',
           region=roi.geometry()
       )
       task[i].start()
       print("end createUniqueUnits task: "+year)
#uncomment the following  to run it
#saveUniqueUnits()

#Once the required datasets (relativeNPP & unique units) available
#we can compute carbon gap
def computeCarbonGap(year):
    relativeNpp = ee.Image("users/myaccount_name/relativeNpp"+str(year))
    uniqueunit = ee.Image("users/myaccount_name/UniqueUnits"+str(year))

    # make neighborhood pixels to image bands
    nbhBands_unitClass=uniqueunit.neighborhoodToBands(neighborhoodKernel)

    #get unique id from segmentation unit
    def getUnitClassBands(bandName):
        return nbhBands_unitClass.select([bandName])
    bandList_unitClass = list(map(getUnitClassBands, (nbhBands_unitClass.bandNames().getInfo())))

    imgCollection_unitClass=ee.ImageCollection.fromImages(bandList_unitClass)

    def maskUnitClassImg(img):
      #using unique_units to mask neighborhood pixel layers
      return img.neq(uniqueunit) #Create a binary mask

    imgCollection_unitClass_masked=imgCollection_unitClass.map(maskUnitClassImg)

    def maskNPPImg(img):
      return img.mask(imgCollection_unitClass_masked.filterMetadata('system:index','equals',img.get('system:index')).first()).rename(['b'])

    nbhBands_npp=relativeNpp.neighborhoodToBands(neighborhoodKernel)
    def getNppBands(bandName):
        return nbhBands_npp.select([bandName])

    bandList_npp = list(map(partial(getNppBands), nbhBands_npp.bandNames().getInfo()))

    imgCollection_npp=ee.ImageCollection.fromImages(bandList_npp)
    imgCollection_npp_masked=imgCollection_npp.map(maskNPPImg)

    focal_pct90 = imgCollection_npp_masked.reduce(reducer=percentile).rename('npp')
    gap=focal_pct90.subtract(relativeNpp)
    mask=gap.gt(ee.Image.constant(0)) # mask out the pixel values that are less than 0
    # pixel value>0 will be kept and value=0 will be masked out; pixel values that are less than 0 will be replaced with 0
    gap=gap.mask(mask).unmask(0) # replace with 0

    print("begin task: "+str(year))
    task = ee.batch.Export.image.toAsset(
        image=gap,
        description='Gap'+str(year),
        assetId='users/myaccount_name/Gap'+str(year),
        scale=500,
        maxPixels=1e13,
        crs='EPSG:4326',
        region=roi.geometry()
    )
    task.start()
    print("end taskï¼š "+str(year))
def computeGAP():
    for i in range(1):
        computeCarbonGap(2001+i)

#uncomment the following  to run it
#computeGAP()

#Once Gap data is available
#doinng statistics based on land cover types
# all statistics are performed based on the save images

#define some global variables for the statistics functions
list=[];vlist=[];vlist_cnt=[];

list_pixelarea=[];vlist_pixelarea=[];vlist_cnt_pixelarea=[]

#compute gap statistics for land cover ids
def computeGapStatisticsForLandCover(year):
    # Combine the mean and standard deviation reducers.
    reducers = ee.Reducer.mean().combine(
      reducer2=ee.Reducer.count(),
      sharedInputs=True
    )
    landcover = landcoverMOD12Q1.filterDate(str(year)+'-1-1',str(year)+ '-12-31').select('LC_Type1').first().clip(roi).reproject('EPSG:4326', None, scaleSize)
    gap= ee.Image("users/myaccount_name/Gap"+str(year))
    #land dvoer 1~17
    for i in range(17):
        mask=landcover.eq(ee.Image.constant(i+1))
        mask=landcover.mask(mask).toByte()
        print('computeGapStatisticsForLandCover process land cover '+str(i+1)+' in '+str(year))
        means = gap.mask(mask).addBands(mask).reduceRegion(
          reducer=reducers.group( #ee.Reducer.mean().group(
            groupField=1,
            groupName='code',
          ),
          geometry=roi.geometry(),
          maxPixels=1e13,
          bestEffort=True
        )
        v=means.getInfo()
        list.append(v)
        print("gap:")
        print(v)
        print('computeGapStatisticsForLandCover end process land cover '+str(i+1)+' in '+str(year))

#compute gap statistics for land cover id for years and save to file
def computeGapStatistics(year):
    list.clear()
    vlist.clear()
    vlist_cnt.clear()
    list_pixelarea.clear()
    vlist_pixelarea.clear()
    vlist_cnt_pixelarea.clear()
    for i in range(17):
        vlist.append(0)
        vlist_cnt.append(0)
        vlist_pixelarea.append(0)
        vlist_cnt_pixelarea.append(0)
    computeGapStatisticsForLandCover(year)
    for indx in range(len(list)):
        tt=list[indx]["groups"][0]
        if tt is not None:
            code=tt["code"]
            vlist[code-1]=tt["mean"]
            #vlist_cnt[code-1]=vlist_cnt[code-1]+tt["count"]

    #for t in list:
    print("------START PRINT-------"+str(year)+"----------------")
    vlist_str=""
    for i in range(17):
        vlist_str+=str(round(vlist[i],1))+"    "
    print ("GAP("+str(year)+"):"+vlist_str)
    f = open("full_dir/statistics.csv", "a")
    f.write("GAP    "+str(year)+"   "+vlist_str+"\n")
    f.close()
    print("--------END PRINT--------"+str(year)+"------------------")

# average NPP for each landcover type
def computeStatisticsNPPForLandCover(year):
    # Combine the mean and standard deviation reducers.
    reducers = ee.Reducer.mean()
    landcover = landcoverMOD12Q1.filterDate(str(year)+'-1-1',str(year)+ '-12-31')\
        .select('LC_Type1').first().clip(roi).reproject('EPSG:4326', None, scaleSize)
    npp= ee.Image("users/myaccount_name/NPP"+str(year))
    #land dvoer 1~17
    for i in range(17):
        mask=landcover.eq(ee.Image.constant(i+1))
        mask=landcover.mask(mask).toByte()
        print('computeStatisticsNPPForLandCover process land cover '+str(i+1)+' in '+str(year))
        means = npp.mask(mask).addBands(mask).reduceRegion(
          reducer=reducers.group( #ee.Reducer.mean().group(
            groupField=1,
            groupName='code',
          ),
          geometry=roi.geometry(),
          maxPixels=1e13,
          bestEffort=True
        )
        v=means.getInfo()
        list.append(v)
        print(v)
        print('computeStatisticsNPPForLandCover end process land cover '+str(i+1)+' in '+str(year))

#compute NPP average for land cover id for years and save to file
def computeStatisticsNPP(year):
    list.clear()
    vlist.clear()
    vlist_cnt.clear()
    for i in range(17):
        vlist.append(0)
        vlist_cnt.append(0)
    computeStatisticsNPPForLandCover(year)
    for t in list:
        tt=t["groups"][0]
        if tt is not None:
            code=tt["code"]
            vlist[code-1]=tt["mean"]
    #for t in list:
    print("------START PRINT-------"+str(year)+"----------------")
    vlist_str=""
    for i in range(17):
        vlist_str+=str(round(vlist[i],1))+"    "
    print ("NPP("+str(year)+"):"+vlist_str)
    f = open("full_dir/statistics.csv", "a")
    f.write("NPP    "+str(year)+"   "+vlist_str+"\n")
    f.close()

    print("--------END PRINT--------"+str(year)+"------------------")


# standard deviation of climate-rectified ONPP for each landcover type
def computeStdDevGapForLandCover(year):
    # Combine the mean and standard deviation reducers.
    reducers = ee.Reducer.stdDev()

    landcover = landcoverMOD12Q1.filterDate(str(year)+'-1-1',str(year)+ '-12-31').select('LC_Type1').first().clip(roi).reproject('EPSG:4326', None, scaleSize)
    relativeNpp= ee.Image("users/myaccount_name/relativeNpp"+str(year)).clip(roi)
    #.reproject('EPSG:4326', None, scaleSize)
    #land dvoer 1~17
    for i in range(17):
        mask=landcover.eq(ee.Image.constant(i+1))
        mask=landcover.mask(mask).toByte()
        print('computeStdDevGapForLandCover process land cover '+str(i+1)+' in '+str(year))
        means = relativeNpp.mask(mask).addBands(mask).reduceRegion(
          reducer=reducers.group( #ee.Reducer.mean().group(
            groupField=1,
            groupName='code',
          ),
          geometry=roi.geometry(),
          maxPixels=1e13,
          bestEffort=True
        )
        v=means.getInfo()
        list.append(v)
        print(v)
        print('computeStdDevGapForLandCover end process land cover '+str(i+1)+' in '+str(year))

#compute NPP average for land cover id for years and save to file
def computeStdDevGap(year):
    list.clear()
    vlist.clear()
    vlist_cnt.clear()
    for i in range(17):
        vlist.append(0)
    computeStdDevGapForLandCover(year)
    for t in list:
        tt=t["groups"][0]
        if tt is not None:
            code=tt["code"]
            vlist[code-1]=tt["stdDev"]
    #for t in list:
    print("------START PRINT-------"+str(year)+"----------------")
    vlist_str=""
    for i in range(17):
        vlist_str+=str(round(vlist[i],1))+"    "
    print ("stdDev("+str(year)+"):"+vlist_str)

    f = open("full_dir/statistics.csv", "a")
    f.write("stdDev    "+str(year)+"    "+vlist_str+"\n")
    f.close()
    print("--------END PRINT--------"+str(year)+"------------------")


# pixel-based gap, or traditional differential gap (=PNPP-ONPP)
def computePixelBasedGapStatisticsForLandCover(year):
    # Combine the mean and standard deviation reducers.
    reducers = ee.Reducer.mean().combine(
      reducer2=ee.Reducer.count(),
      sharedInputs=True
    )
    landcover = landcoverMOD12Q1.filterDate(str(year)+'-1-1',str(year)+ '-12-31').select('LC_Type1').first().clip(roi).reproject('EPSG:4326', None, scaleSize)
    gap= ee.Image("users/myaccount_name/DiffGap"+str(year))
    #land dvoer 1~17
    for i in range(17):
        mask=landcover.eq(ee.Image.constant(i+1))
        mask=landcover.mask(mask).toByte()
        print('computePixelBasedGapStatisticsForLandCover process land cover '+str(i+1)+' in '+str(year))
        means = gap.mask(mask).addBands(mask).reduceRegion(
          reducer=reducers.group( #ee.Reducer.mean().group(
            groupField=1,
            groupName='code',
          ),
          geometry=roi.geometry(),
          maxPixels=1e13,
          bestEffort=True
        )
        v=means.getInfo()
        list.append(v)
        print("gap:")
        print(v)
        print('computePixelBasedGapStatisticsForLandCover end process land cover '+str(i+1)+' in '+str(year))

#compute gap statistics for land cover id for years and save to file
def computePixelBasedGapStatistics(year):
    list.clear()
    vlist.clear()
    vlist_cnt.clear()
    list_pixelarea.clear()
    vlist_pixelarea.clear()
    vlist_cnt_pixelarea.clear()
    for i in range(17):
        vlist.append(0)
        vlist_cnt.append(0)
        vlist_pixelarea.append(0)
        vlist_cnt_pixelarea.append(0)
    computePixelBasedGapStatisticsForLandCover(year)
    for indx in range(len(list)):
        tt=list[indx]["groups"][0]
        if tt is not None:
            code=tt["code"]
            vlist[code-1]=tt["mean"]
            #vlist_cnt[code-1]=vlist_cnt[code-1]+tt["count"]

    #for t in list:
    print("------START PRINT-------"+str(year)+"----------------")
    vlist_str=""
    for i in range(17):
        vlist_str+=str(round(vlist[i],1))+"    "
    print ("DiffGAP("+str(year)+"):"+vlist_str)
    f = open("full_dir/statistics.csv", "a")
    f.write("DiffGAP    "+str(year)+"   "+vlist_str+"\n")
    f.close()
    print("--------END PRINT--------"+str(year)+"------------------")


# compute area of each land cover id for a year
def computeLandcoverArea(year):
    reducers = ee.Reducer.sum()
    pixelArea= ee.Image("users/myaccount_name/pixelArea")
    landcover = landcoverMOD12Q1.filterDate(str(year)+'-1-1',str(year)+ '-12-31').select('LC_Type1').first().clip(roi).reproject('EPSG:4326', None, scaleSize)
    for i in range(17):
        mask=landcover.eq(ee.Image.constant(i+1))
        mask=landcover.mask(mask).toByte()
        print('computeLandcoverArea process land cover '+str(i+1)+' in '+str(year))
        count = pixelArea.mask(mask).addBands(mask).reduceRegion(
          reducer=reducers.group( #ee.Reducer.mean().group(
            groupField=1,
            groupName='code',
          ),
          geometry=roi.geometry(),
          maxPixels=1e13,
          bestEffort=True
        )
        v=count.getInfo()
        list.append(v)
        print("area:")
        print(v)
        print('computeLandcoverArea end process land cover '+str(i+1)+' in '+str(year))

# save area of each land cover to file
def computeLandCoverAreaStatistics(year):
    list.clear()
    vlist.clear()
    vlist_cnt.clear()
    list_pixelarea.clear()
    vlist_pixelarea.clear()
    vlist_cnt_pixelarea.clear()
    for i in range(17):
        vlist.append(0)
        vlist_cnt.append(0)
        vlist_pixelarea.append(0)
        vlist_cnt_pixelarea.append(0)
    computeLandcoverArea(year)
    for indx in range(len(list)):
        tt=list[indx]["groups"][0]
        if tt is not None:
            code=tt["code"]
            vlist[code-1]=tt["sum"]
            #vlist_cnt[code-1]=vlist_cnt[code-1]+tt["count"]

    #for t in list:
    print("------START PRINT-------"+str(year)+"----------------")
    vlist_str=""
    for i in range(17):
        vlist_str+=str(round(vlist[i],0))+"    "
    print ("LandArea("+str(year)+"):"+vlist_str)
    f = open("full_dir/statistics.csv", "a")
    f.write("LandArea    "+str(year)+"   "+vlist_str+"\n")
    f.close()
    print("--------END PRINT--------"+str(year)+"------------------")


# now we can output the result in one click
def processAll():
    for i in range(18):
        print ("compute year: "+str(2001+i))
        computeGapStatistics(i+2001)
        computeStatisticsNPP(i+2001)
        computeStdDevGap(i+2001)
        computePixelBasedGapStatistics(i+2001)
        computeLandCoverAreaStatistics(i+2001)
        estimateDistance(2005)

#uncomment the following  to run it
#processAll()
